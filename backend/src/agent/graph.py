from typing import TypedDict, List, Optional
from typing import TypedDict, List, Annotated
from langgraph.graph import StateGraph, END
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.utilities import GoogleSearchAPIWrapper
from langchain.schema import HumanMessage, SystemMessage
import os
from datetime import datetime
import json
import re

# ç‹€æ…‹å®šç¾©
class AgentState(TypedDict):
    user_query: str
    search_queries: List[str]
    search_results: List[dict]
    all_search_results: List[dict]  # ç´¯ç©æ‰€æœ‰æœç´¢çµæœ
    reflection: str
    knowledge_gaps: List[str]
    final_answer: str
    sources: List[str]
    iteration_count: int
    max_iterations: int
    search_summary: str  # æœç´¢çµæœæ‘˜è¦
    confidence_score: float  # ç­”æ¡ˆä¿¡å¿ƒåº¦

# åˆå§‹åŒ–å‡½æ•°
def get_llm():
    """è·å–LLMå®ä¾‹"""
    return ChatGoogleGenerativeAI(
        model="gemini-1.5-pro",
        google_api_key=os.getenv("GEMINI_API_KEY"),
        temperature=0.1
    )

def get_search():
    """è·å–æœç´¢å·¥å…·å®ä¾‹"""
    google_api_key = os.getenv("GOOGLE_API_KEY")
    google_cse_id = os.getenv("GOOGLE_CSE_ID")
    
    if not google_api_key or not google_cse_id or google_api_key == "your_google_search_api_key_here":
        print("è­¦å‘Šï¼šGoogleæœç´¢APIæœªé…ç½®ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæœç´¢")
        return None
    
    return GoogleSearchAPIWrapper(
        google_api_key=google_api_key,
        google_cse_id=google_cse_id,
        k=5
    )

# ç‰©æµå°ˆå®¶ç³»çµ±æç¤ºè©
LOGISTICS_SYSTEM_PROMPT = """
ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ç‰©æµå’Œèˆªé‹å°ˆå®¶åŠ©æ‰‹ï¼Œå…·æœ‰ä»¥ä¸‹å°ˆæ¥­é ˜åŸŸçš„æ·±åº¦çŸ¥è­˜ï¼š

ğŸš¢ æ ¸å¿ƒå°ˆæ¥­é ˜åŸŸï¼š
- è²¨æ«ƒé‹è¼¸å’Œå³æ™‚è¿½è¹¤ç³»çµ±
- åœ‹éš›èˆªé‹è·¯ç·šå’Œæ¸¯å£æ“ä½œ
- æµ·é—œæ¸…é—œå’Œåœ‹éš›è²¿æ˜“æ³•è¦
- ä¾›æ‡‰éˆç®¡ç†å’Œå„ªåŒ–ç­–ç•¥
- ç‰©æµæˆæœ¬åˆ†æå’Œè²»ç‡æ¯”è¼ƒ
- ä¸»è¦èˆªé‹å…¬å¸æœå‹™è©•ä¼°ï¼ˆMaersk, COSCO, MSC, ONE, HMMç­‰ï¼‰
- å…¨çƒæ¸¯å£ä¿¡æ¯å’Œèˆªç·šè¦åŠƒ
- è²¨é‹ä¿éšªå’Œé¢¨éšªç®¡ç†
- æ•¸ä½åŒ–ç‰©æµè§£æ±ºæ–¹æ¡ˆ

ğŸ’¡ å›ç­”åŸå‰‡ï¼š
1. æä¾›æº–ç¢ºã€å¯¦ç”¨ä¸”å¯æ“ä½œçš„å»ºè­°
2. åŒ…å«å…·é«”æ•¸æ“šã€æ™‚é–“è¡¨ã€è²»ç”¨ç­‰è©³ç´°ä¿¡æ¯
3. å¼•ç”¨æœ€æ–°çš„è¡Œæ¥­è³‡è¨Šå’Œæ³•è¦è®ŠåŒ–
4. æä¾›å¤šå€‹è§£æ±ºæ–¹æ¡ˆé¸é …ä¾›æ¯”è¼ƒ
5. æ˜ç¢ºæŒ‡å‡ºéœ€è¦é€²ä¸€æ­¥æŸ¥è©¢çš„æœ€æ–°è³‡æ–™
6. ä½¿ç”¨å°ˆæ¥­è¡“èªä½†ä¿æŒæ˜“æ‡‚çš„è§£é‡‹

ğŸ¯ ç‰¹åˆ¥é—œæ³¨ï¼š
- å³æ™‚æ€§è³‡è¨Šï¼ˆè²»ç‡ã€èˆ¹æœŸã€æ¸¯å£ç‹€æ³ï¼‰
- æ³•è¦åˆè¦æ€§å’Œé¢¨éšªæé†’
- æˆæœ¬æ•ˆç›Šåˆ†æå’Œå„ªåŒ–å»ºè­°
- æ•¸ä½åŒ–å·¥å…·å’Œå¹³å°æ¨è–¦
"""

def generate_search_queries(state: AgentState) -> AgentState:
    """åŸºæ–¼ç”¨æˆ¶æŸ¥è©¢ç”Ÿæˆæœç´¢é—œéµè©"""
    user_query = state["user_query"]
    iteration_count = state.get("iteration_count", 0)
    previous_results = state.get("all_search_results", [])
    
    # æ ¹æ“šè¿­ä»£æ¬¡æ•¸èª¿æ•´æœç´¢ç­–ç•¥
    if iteration_count == 0:
        # ç¬¬ä¸€æ¬¡æœç´¢ï¼šç”Ÿæˆå»£æ³›çš„åŸºç¤æŸ¥è©¢
        strategy = "ç”Ÿæˆ3-4å€‹å»£æ³›çš„åŸºç¤æœç´¢è©ï¼Œæ¶µè“‹å•é¡Œçš„ä¸»è¦æ–¹é¢"
    else:
        # å¾ŒçºŒæœç´¢ï¼šåŸºæ–¼å·²æœ‰çµæœç”Ÿæˆæ›´å…·é«”çš„æŸ¥è©¢
        previous_summary = "\n".join([f"- {r.get('query', '')}: {r.get('results', '')[:200]}..." for r in previous_results[-3:]])
        strategy = f"åŸºæ–¼å·²æœ‰æœç´¢çµæœï¼Œç”Ÿæˆ2-3å€‹æ›´å…·é«”å’Œæ·±å…¥çš„æœç´¢è©ä¾†å¡«è£œçŸ¥è­˜ç¼ºå£\n\nå·²æœ‰çµæœæ‘˜è¦ï¼š\n{previous_summary}"
    
    prompt = f"""
    {strategy}
    
    ç”¨æˆ¶æŸ¥è©¢ï¼š{user_query}
    
    è«‹ç”Ÿæˆé‡å°ç‰©æµã€èˆªé‹ã€è²¨æ«ƒè¿½è¹¤ç­‰é ˜åŸŸçš„å°ˆæ¥­æœç´¢è©ã€‚è¦æ±‚ï¼š
    1. æ¯å€‹æœç´¢è©æ‡‰è©²ç°¡æ½”ä¸”å…·é«”
    2. åŒ…å«ç›¸é—œçš„å°ˆæ¥­è¡“èªå’Œé—œéµè©
    3. è€ƒæ…®æ™‚æ•ˆæ€§ï¼ˆå¦‚"2024å¹´"ã€"æœ€æ–°"ç­‰ï¼‰
    4. ç”¨æ›è¡Œç¬¦åˆ†éš”ï¼Œæ¯è¡Œä¸€å€‹æœç´¢è©
    
    æœç´¢é—œéµè©ï¼š
    """
    
    llm = get_llm()
    response = llm.invoke([SystemMessage(content=LOGISTICS_SYSTEM_PROMPT), HumanMessage(content=prompt)])
    search_queries = [q.strip() for q in response.content.split('\n') if q.strip() and not q.startswith('-')]
    
    # æ¸…ç†å’Œå„ªåŒ–æœç´¢è©
    cleaned_queries = []
    for query in search_queries[:4]:  # é™åˆ¶æœ€å¤š4å€‹æŸ¥è©¢
        # ç§»é™¤å¤šé¤˜çš„æ¨™é»å’Œæ ¼å¼
        cleaned_query = re.sub(r'^[\d\.\-\*\s]+', '', query).strip()
        if len(cleaned_query) > 5:  # ç¢ºä¿æŸ¥è©¢è©æœ‰æ„ç¾©
            cleaned_queries.append(cleaned_query)
    
    state["search_queries"] = cleaned_queries
    return state

def web_search(state: AgentState) -> AgentState:
    """åŸ·è¡Œç¶²è·¯æœç´¢"""
    search_results = []
    all_results = state.get("all_search_results", [])
    search = get_search()
    
    for query in state["search_queries"]:
        try:
            print(f"æœç´¢æŸ¥è©¢: {query}")
            
            if search is None:
                # æ¨¡æ“¬æœç´¢çµæœ
                print("ä½¿ç”¨æ¨¡æ“¬æœç´¢çµæœ")
                results = f"é—œæ–¼ {query} çš„æ¨¡æ“¬æœç´¢çµæœã€‚ç”±æ–¼Googleæœç´¢APIæœªé…ç½®ï¼Œé€™æ˜¯ä¸€å€‹ç¤ºä¾‹çµæœã€‚"
                processed_results = results
            else:
                # åŸ·è¡ŒçœŸå¯¦æœç´¢
                results = search.run(query)
                # è™•ç†å’Œæ¸…ç†æœç´¢çµæœ
                processed_results = process_search_results(results)
            
            result_entry = {
                "query": query,
                "results": results,
                "processed_results": processed_results,
                "timestamp": datetime.now().isoformat(),
                "result_count": len(processed_results.split('\n')) if processed_results else 0
            }
            
            search_results.append(result_entry)
            all_results.append(result_entry)
            
        except Exception as e:
            print(f"æœç´¢éŒ¯èª¤ '{query}': {e}")
            # æ·»åŠ éŒ¯èª¤è¨˜éŒ„
            error_entry = {
                "query": query,
                "results": f"æœç´¢å¤±æ•—: {str(e)}",
                "processed_results": "",
                "timestamp": datetime.now().isoformat(),
                "result_count": 0,
                "error": True
            }
            search_results.append(error_entry)
            continue
    
    state["search_results"] = search_results
    state["all_search_results"] = all_results
    return state

def process_search_results(raw_results: str) -> str:
    """è™•ç†å’Œæ¸…ç†æœç´¢çµæœ"""
    if not raw_results:
        return ""
    
    # ç§»é™¤éå¤šçš„ç©ºç™½å’Œé‡è¤‡å…§å®¹
    lines = raw_results.split('\n')
    processed_lines = []
    
    for line in lines:
        line = line.strip()
        if line and len(line) > 10:  # éæ¿¾å¤ªçŸ­çš„è¡Œ
            # ç§»é™¤é‡è¤‡çš„URLå’Œç„¡ç”¨ä¿¡æ¯
            if not any(skip_word in line.lower() for skip_word in ['cookie', 'privacy policy', 'terms of service', 'advertisement']):
                processed_lines.append(line)
    
    return '\n'.join(processed_lines[:10])  # é™åˆ¶çµæœé•·åº¦

def reflect_and_analyze(state: AgentState) -> AgentState:
    """åˆ†ææœç´¢çµæœä¸¦è­˜åˆ¥çŸ¥è­˜ç¼ºå£"""
    user_query = state["user_query"]
    search_results = state["search_results"]
    iteration_count = state.get("iteration_count", 0)
    all_results = state.get("all_search_results", [])
    
    # æ•´ç†ç•¶å‰æœç´¢çµæœ
    current_results_summary = ""
    successful_searches = 0
    
    for result in search_results:
        if not result.get("error", False):
            successful_searches += 1
            processed = result.get("processed_results", result.get("results", ""))
            current_results_summary += f"\nğŸ” æŸ¥è©¢: {result['query']}\nğŸ“„ çµæœ: {processed[:300]}...\n"
    
    # æ•´ç†æ‰€æœ‰æ­·å²çµæœçš„æ‘˜è¦
    all_results_summary = ""
    if len(all_results) > len(search_results):
        previous_results = all_results[:-len(search_results)]
        all_results_summary = f"\n\nğŸ“š å…ˆå‰æœç´¢æ‘˜è¦ï¼š\n"
        for result in previous_results[-3:]:  # åªé¡¯ç¤ºæœ€è¿‘3å€‹
            if not result.get("error", False):
                all_results_summary += f"- {result['query']}: {result.get('processed_results', '')[:100]}...\n"
    
    prompt = f"""
    ä½œç‚ºç‰©æµå°ˆå®¶ï¼Œè«‹æ·±åº¦åˆ†æä»¥ä¸‹æœç´¢çµæœï¼Œè©•ä¼°æ˜¯å¦è¶³ä»¥å›ç­”ç”¨æˆ¶æŸ¥è©¢ã€‚
    
    ğŸ‘¤ ç”¨æˆ¶æŸ¥è©¢ï¼š{user_query}
    
    ğŸ”„ ç•¶å‰è¿­ä»£ï¼šç¬¬ {iteration_count + 1} æ¬¡æœç´¢
    âœ… æˆåŠŸæœç´¢ï¼š{successful_searches}/{len(search_results)} å€‹æŸ¥è©¢
    
    ğŸ“Š ç•¶å‰æœç´¢çµæœï¼š{current_results_summary}
    {all_results_summary}
    
    è«‹é€²è¡Œä»¥ä¸‹åˆ†æï¼š
    
    1. **ä¿¡æ¯å®Œæ•´æ€§è©•ä¼°** (0-100åˆ†)ï¼š
       - æ˜¯å¦æ¶µè“‹äº†ç”¨æˆ¶å•é¡Œçš„æ ¸å¿ƒè¦ç´ ï¼Ÿ
       - ä¿¡æ¯çš„æº–ç¢ºæ€§å’Œæ™‚æ•ˆæ€§å¦‚ä½•ï¼Ÿ
       - æ˜¯å¦æœ‰å…·é«”çš„æ•¸æ“šã€è²»ç‡ã€æ™‚é–“ç­‰é—œéµä¿¡æ¯ï¼Ÿ
    
    2. **çŸ¥è­˜ç¼ºå£è­˜åˆ¥**ï¼š
       - é‚„ç¼ºå°‘å“ªäº›é—œéµä¿¡æ¯ï¼Ÿ
       - å“ªäº›æ–¹é¢éœ€è¦æ›´æ·±å…¥çš„æœç´¢ï¼Ÿ
       - æ˜¯å¦éœ€è¦æ›´å…·é«”çš„å°ˆæ¥­è¡“èªæœç´¢ï¼Ÿ
    
    3. **æœç´¢ç­–ç•¥å»ºè­°**ï¼š
       - å¦‚æœéœ€è¦ç¹¼çºŒæœç´¢ï¼Œæ‡‰è©²é—œæ³¨å“ªäº›æ–¹å‘ï¼Ÿ
       - å»ºè­°çš„æœç´¢é—œéµè©é¡å‹ï¼Ÿ
    
    è«‹æŒ‰ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š
    
    **ä¿¡å¿ƒåº¦åˆ†æ•¸**: [0-100]
    **è©•ä¼°çµæœ**: [SUFFICIENT/NEEDS_MORE_INFO]
    **ä¸»è¦ç™¼ç¾**: [ç°¡è¦ç¸½çµå·²æ‰¾åˆ°çš„é—œéµä¿¡æ¯]
    **çŸ¥è­˜ç¼ºå£**: [åˆ—å‡ºå…·é«”ç¼ºå¤±çš„ä¿¡æ¯ï¼Œå¦‚æœæ²’æœ‰å‰‡å¯«"ç„¡"]
    **å»ºè­°æœç´¢æ–¹å‘**: [å¦‚æœéœ€è¦æ›´å¤šæœç´¢ï¼Œå»ºè­°çš„æ–¹å‘]
    """
    
    llm = get_llm()
    response = llm.invoke([SystemMessage(content=LOGISTICS_SYSTEM_PROMPT), HumanMessage(content=prompt)])
    reflection = response.content
    
    state["reflection"] = reflection
    
    # è§£æä¿¡å¿ƒåº¦åˆ†æ•¸
    confidence_match = re.search(r'ä¿¡å¿ƒåº¦åˆ†æ•¸.*?[ï¼š:]\s*(\d+)', reflection)
    confidence_score = float(confidence_match.group(1)) / 100 if confidence_match else 0.5
    state["confidence_score"] = confidence_score
    
    # æª¢æŸ¥æ˜¯å¦éœ€è¦æ›´å¤šæœç´¢
    needs_more = "NEEDS_MORE_INFO" in reflection.upper() or "éœ€è¦æ›´å¤š" in reflection
    sufficient = "SUFFICIENT" in reflection.upper() or confidence_score >= 0.8
    
    if sufficient and not needs_more:
        state["knowledge_gaps"] = []
    else:
        # æå–çŸ¥è­˜ç¼ºå£ä¸¦ç”Ÿæˆæ–°çš„æœç´¢æŸ¥è©¢
        gaps_prompt = f"""
        åŸºæ–¼ä»¥ä¸‹åˆ†æçµæœï¼Œç”Ÿæˆ2-3å€‹ç²¾ç¢ºçš„æœç´¢æŸ¥è©¢ä¾†å¡«è£œçŸ¥è­˜ç¼ºå£ï¼š
        
        åˆ†æçµæœï¼š{reflection}
        
        åŸå§‹ç”¨æˆ¶æŸ¥è©¢ï¼š{user_query}
        
        è«‹ç”Ÿæˆå…·é«”ã€å°ˆæ¥­çš„æœç´¢é—œéµè©ï¼Œè¦æ±‚ï¼š
        1. é‡å°è­˜åˆ¥å‡ºçš„çŸ¥è­˜ç¼ºå£
        2. ä½¿ç”¨å°ˆæ¥­çš„ç‰©æµè¡“èª
        3. åŒ…å«æ™‚æ•ˆæ€§é—œéµè©ï¼ˆå¦‚"2024"ã€"æœ€æ–°"ã€"ç•¶å‰"ï¼‰
        4. æ¯è¡Œä¸€å€‹æœç´¢è©ï¼Œä¸è¦ç·¨è™Ÿæˆ–ç¬¦è™Ÿ
        
        æœç´¢é—œéµè©ï¼š
        """
        
        llm = get_llm()
        gaps_response = llm.invoke([SystemMessage(content=LOGISTICS_SYSTEM_PROMPT), HumanMessage(content=gaps_prompt)])
        knowledge_gaps = [q.strip() for q in gaps_response.content.split('\n') if q.strip() and len(q.strip()) > 5]
        
        # æ¸…ç†æœç´¢è©
        cleaned_gaps = []
        for gap in knowledge_gaps[:3]:  # æœ€å¤š3å€‹
            cleaned_gap = re.sub(r'^[\d\.\-\*\s]+', '', gap).strip()
            if cleaned_gap and len(cleaned_gap) > 5:
                cleaned_gaps.append(cleaned_gap)
        
        state["knowledge_gaps"] = cleaned_gaps
    
    # ç”Ÿæˆæœç´¢çµæœæ‘˜è¦
    summary_prompt = f"""
    è«‹ç‚ºä»¥ä¸‹æœç´¢çµæœç”Ÿæˆä¸€å€‹ç°¡æ½”çš„æ‘˜è¦ï¼ˆ100å­—ä»¥å…§ï¼‰ï¼š
    
    {current_results_summary}
    
    æ‘˜è¦æ‡‰è©²çªå‡ºé—œéµç™¼ç¾å’Œé‡è¦ä¿¡æ¯ã€‚
    """
    
    llm = get_llm()
    summary_response = llm.invoke([HumanMessage(content=summary_prompt)])
    state["search_summary"] = summary_response.content
    
    return state

def generate_final_answer(state: AgentState) -> AgentState:
    """ç”Ÿæˆæœ€çµ‚ç­”æ¡ˆ"""
    user_query = state["user_query"]
    all_search_results = state.get("all_search_results", [])
    search_summary = state.get("search_summary", "")
    confidence_score = state.get("confidence_score", 0.5)
    reflection = state.get("reflection", "")
    
    # æ•´ç†æ‰€æœ‰æœç´¢çµæœï¼ŒæŒ‰ç›¸é—œæ€§å’Œè³ªé‡æ’åº
    relevant_results = []
    sources = []
    
    for result in all_search_results:
        if not result.get("error", False) and result.get("result_count", 0) > 0:
            processed = result.get("processed_results", result.get("results", ""))
            if processed and len(processed.strip()) > 50:  # ç¢ºä¿æœ‰å¯¦è³ªå…§å®¹
                relevant_results.append({
                    "query": result["query"],
                    "content": processed[:500],  # é™åˆ¶é•·åº¦
                    "timestamp": result.get("timestamp", "")
                })
                sources.append(result["query"])
    
    # æ§‹å»ºç¶œåˆçš„æœç´¢çµæœæ‘˜è¦
    results_summary = "\n".join([
        f"ğŸ“‹ {i+1}. æœç´¢: {r['query']}\n   çµæœ: {r['content'][:200]}...\n"
        for i, r in enumerate(relevant_results[:5])  # æœ€å¤š5å€‹æœ€ç›¸é—œçš„çµæœ
    ])
    
    prompt = f"""
    ä½œç‚ºå°ˆæ¥­çš„ç‰©æµå’Œèˆªé‹å°ˆå®¶ï¼Œè«‹åŸºæ–¼æœç´¢çµæœç‚ºç”¨æˆ¶æä¾›å…¨é¢ã€æº–ç¢ºçš„ç­”æ¡ˆã€‚
    
    ğŸ‘¤ **ç”¨æˆ¶æŸ¥è©¢**: {user_query}
    
    ğŸ“Š **ä¿¡å¿ƒåº¦**: {confidence_score:.0%}
    ğŸ“ **æœç´¢æ‘˜è¦**: {search_summary}
    
    ğŸ” **è©³ç´°æœç´¢çµæœ**:
    {results_summary}
    
    ğŸ“‹ **åˆ†ææ´å¯Ÿ**:
    {reflection}
    
    è«‹æä¾›çµæ§‹åŒ–çš„å°ˆæ¥­å›ç­”ï¼ŒåŒ…å«ä»¥ä¸‹è¦ç´ ï¼š
    
    ## ğŸ“‹ æ ¸å¿ƒç­”æ¡ˆ
    [ç›´æ¥å›ç­”ç”¨æˆ¶çš„ä¸»è¦å•é¡Œï¼Œè¦å…·é«”æ˜ç¢º]
    
    ## ğŸ“Š é—œéµä¿¡æ¯
    [æä¾›å…·é«”æ•¸æ“šï¼Œå¦‚ï¼šè²»ç‡ã€æ™‚é–“ã€è¦æ ¼ã€æµç¨‹ç­‰]
    
    ## ğŸ’¡ å°ˆæ¥­å»ºè­°
    [åŸºæ–¼è¡Œæ¥­ç¶“é©—çš„å¯¦ç”¨å»ºè­°å’Œæœ€ä½³å¯¦è¸]
    
    ## ğŸ”— ä¸‹ä¸€æ­¥è¡Œå‹•
    [å…·é«”çš„æ“ä½œæ­¥é©Ÿæˆ–è¯ç¹«æ–¹å¼]
    
    ## âš ï¸ æ³¨æ„äº‹é …
    [é‡è¦çš„é¢¨éšªæé†’æˆ–åˆè¦è¦æ±‚]
    
    ## ğŸ“š å»¶ä¼¸è³‡æº
    [ç›¸é—œçš„å®˜æ–¹ç¶²ç«™ã€å·¥å…·æˆ–é€²ä¸€æ­¥æŸ¥è©¢å»ºè­°]
    
    **å›ç­”è¦æ±‚**:
    - ä½¿ç”¨å°ˆæ¥­ä½†æ˜“æ‡‚çš„èªè¨€
    - æä¾›å¯æ“ä½œçš„å…·é«”å»ºè­°
    - æ¨™è¨»ä¿¡æ¯çš„æ™‚æ•ˆæ€§å’Œå¯é æ€§
    - å¦‚æœä¿¡æ¯ä¸å®Œæ•´ï¼Œæ˜ç¢ºèªªæ˜é™åˆ¶
    - ä¿æŒå®¢è§€å’Œæº–ç¢º
    """
    
    response = llm.invoke([SystemMessage(content=LOGISTICS_SYSTEM_PROMPT), HumanMessage(content=prompt)])
    
    # å¾Œè™•ç†ç­”æ¡ˆï¼Œæ·»åŠ ä¿¡å¿ƒåº¦å’Œå…è²¬è²æ˜
    final_answer = response.content
    
    # æ·»åŠ ä¿¡å¿ƒåº¦æŒ‡ç¤º
    if confidence_score < 0.7:
        confidence_note = "\n\nâš ï¸ **ä¿¡å¿ƒåº¦æé†’**: ç”±æ–¼æœç´¢çµæœæœ‰é™ï¼Œéƒ¨åˆ†ä¿¡æ¯å¯èƒ½éœ€è¦é€²ä¸€æ­¥ç¢ºèªã€‚å»ºè­°è¯ç¹«ç›¸é—œæ©Ÿæ§‹ç²å–æœ€æ–°æº–ç¢ºä¿¡æ¯ã€‚"
        final_answer += confidence_note
    
    # æ·»åŠ æ™‚æ•ˆæ€§æé†’
    timestamp_note = f"\n\nğŸ•’ **ä¿¡æ¯æ™‚æ•ˆ**: ä»¥ä¸Šä¿¡æ¯åŸºæ–¼ {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')} çš„æœç´¢çµæœï¼Œç‰©æµè¡Œæ¥­ä¿¡æ¯è®ŠåŒ–è¼ƒå¿«ï¼Œè«‹ä»¥å®˜æ–¹æœ€æ–°å…¬å‘Šç‚ºæº–ã€‚"
    final_answer += timestamp_note
    
    state["final_answer"] = final_answer
    state["sources"] = sources
    
    return state

def should_continue(state: AgentState) -> str:
    """æ±ºå®šæ˜¯å¦ç¹¼çºŒæœç´¢"""
    iteration_count = state.get("iteration_count", 0)
    max_iterations = state.get("max_iterations", 3)
    knowledge_gaps = state.get("knowledge_gaps", [])
    confidence_score = state.get("confidence_score", 0.0)
    
    # æª¢æŸ¥æ˜¯å¦é”åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•¸
    if iteration_count >= max_iterations:
        print(f"é”åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•¸ ({max_iterations})ï¼Œç”Ÿæˆæœ€çµ‚ç­”æ¡ˆ")
        return "generate_answer"
    
    # æª¢æŸ¥æ˜¯å¦æ²’æœ‰çŸ¥è­˜ç¼ºå£
    if not knowledge_gaps:
        print("æ²’æœ‰ç™¼ç¾çŸ¥è­˜ç¼ºå£ï¼Œç”Ÿæˆæœ€çµ‚ç­”æ¡ˆ")
        return "generate_answer"
    
    # æª¢æŸ¥ä¿¡å¿ƒåº¦æ˜¯å¦è¶³å¤ é«˜
    if confidence_score >= 0.85:
        print(f"ä¿¡å¿ƒåº¦è¶³å¤ é«˜ ({confidence_score:.0%})ï¼Œç”Ÿæˆæœ€çµ‚ç­”æ¡ˆ")
        return "generate_answer"
    
    # æª¢æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„æœç´¢çµæœ
    all_results = state.get("all_search_results", [])
    successful_results = [r for r in all_results if not r.get("error", False) and r.get("result_count", 0) > 0]
    
    if len(successful_results) == 0 and iteration_count > 0:
        print("æ²’æœ‰æˆåŠŸçš„æœç´¢çµæœï¼Œç”Ÿæˆæœ€çµ‚ç­”æ¡ˆ")
        return "generate_answer"
    
    print(f"ç¹¼çºŒæœç´¢ (ç¬¬ {iteration_count + 1} æ¬¡è¿­ä»£ï¼Œä¿¡å¿ƒåº¦: {confidence_score:.0%})")
    return "continue_search"

def continue_search(state: AgentState) -> AgentState:
    """ç¹¼çºŒæœç´¢ä»¥å¡«è£œçŸ¥è­˜ç¼ºå£"""
    knowledge_gaps = state.get("knowledge_gaps", [])
    iteration_count = state.get("iteration_count", 0)
    
    # æ›´æ–°æœç´¢æŸ¥è©¢ç‚ºçŸ¥è­˜ç¼ºå£
    state["search_queries"] = knowledge_gaps
    state["iteration_count"] = iteration_count + 1
    
    print(f"é–‹å§‹ç¬¬ {state['iteration_count']} æ¬¡æœç´¢ï¼ŒæŸ¥è©¢: {knowledge_gaps}")
    
    return state

# åˆå§‹åŒ–ç‹€æ…‹å‡½æ•¸
def initialize_state(user_query: str, max_iterations: int = 3) -> AgentState:
    """åˆå§‹åŒ–agentç‹€æ…‹"""
    return {
        "user_query": user_query,
        "search_queries": [],
        "search_results": [],
        "all_search_results": [],
        "reflection": "",
        "knowledge_gaps": [],
        "final_answer": "",
        "sources": [],
        "iteration_count": 0,
        "max_iterations": max_iterations,
        "search_summary": "",
        "confidence_score": 0.0
    }

# æ§‹å»ºåœ–
def create_logistics_agent():
    """å‰µå»ºç‰©æµAI agentå·¥ä½œæµç¨‹"""
    workflow = StateGraph(AgentState)
    
    # æ·»åŠ ç¯€é»
    workflow.add_node("generate_queries", generate_search_queries)
    workflow.add_node("search", web_search)
    workflow.add_node("reflect", reflect_and_analyze)
    workflow.add_node("continue_search", continue_search)
    workflow.add_node("generate_answer", generate_final_answer)
    
    # è¨­ç½®é‚Š
    workflow.set_entry_point("generate_queries")
    workflow.add_edge("generate_queries", "search")
    workflow.add_edge("search", "reflect")
    
    # æ¢ä»¶é‚Šï¼šæ±ºå®šæ˜¯å¦ç¹¼çºŒæœç´¢
    workflow.add_conditional_edges(
        "reflect",
        should_continue,
        {
            "continue_search": "continue_search",
            "generate_answer": "generate_answer"
        }
    )
    
    workflow.add_edge("continue_search", "search")
    workflow.add_edge("generate_answer", END)
    
    return workflow.compile()

# å‰µå»ºagentå¯¦ä¾‹
logistics_agent = create_logistics_agent()

# ä¾¿åˆ©å‡½æ•¸ï¼šé‹è¡Œagent
def run_logistics_agent(user_query: str, max_iterations: int = 3) -> dict:
    """é‹è¡Œç‰©æµAI agent"""
    try:
        # åˆå§‹åŒ–ç‹€æ…‹
        initial_state = initialize_state(user_query, max_iterations)
        
        # é‹è¡Œå·¥ä½œæµç¨‹
        result = logistics_agent.invoke(initial_state)
        
        return {
            "success": True,
            "answer": result.get("final_answer", "ç„¡æ³•ç”Ÿæˆç­”æ¡ˆ"),
            "sources": result.get("sources", []),
            "search_queries": [r.get("query", "") for r in result.get("all_search_results", [])],
            "confidence_score": result.get("confidence_score", 0.0),
            "iteration_count": result.get("iteration_count", 0),
            "search_summary": result.get("search_summary", "")
        }
        
    except Exception as e:
        print(f"Agenté‹è¡ŒéŒ¯èª¤: {e}")
        return {
            "success": False,
            "answer": f"æŠ±æ­‰ï¼Œè™•ç†æ‚¨çš„è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}",
            "sources": [],
            "search_queries": [],
            "confidence_score": 0.0,
            "iteration_count": 0,
            "search_summary": "",
            "error": str(e)
        }